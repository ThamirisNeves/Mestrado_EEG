{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L5XnQRM1y-R3",
    "outputId": "cb2eaa1f-0f9d-477d-ef05-0404aa7ce3ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting mne\n",
      "  Downloading mne-1.0.3-py3-none-any.whl (7.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.5 MB 19.8 MB/s \n",
      "\u001b[?25hRequirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.7/dist-packages (from mne) (1.6.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from mne) (21.3)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from mne) (1.21.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from mne) (4.64.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from mne) (3.2.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from mne) (2.11.3)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from mne) (1.4.1)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from mne) (4.4.2)\n",
      "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.5->mne) (1.4.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.5->mne) (2.23.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->mne) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2022.6.15)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.10)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->mne) (2.0.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mne) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mne) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mne) (1.4.3)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->mne) (4.1.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->mne) (1.15.0)\n",
      "Installing collected packages: mne\n",
      "Successfully installed mne-1.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install mne\n",
    "!pip install braindecode\n",
    "!pip install pyriemann\n",
    "!pip install moabb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "leuFGoutycGG"
   },
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import tqdm\n",
    "\n",
    "#Caminho é o local onde se encontra o arquivo EEGModels.py\n",
    "\n",
    "#sys.path.append(os.path.abspath(caminho))\n",
    " \n",
    "from braindecode.preprocessing  import create_windows_from_events\n",
    "from braindecode.preprocessing  import exponential_moving_standardize\n",
    "from braindecode.datasets       import MOABBDataset\n",
    "from braindecode.preprocessing  import preprocess\n",
    "from braindecode.preprocessing  import Preprocessor\n",
    "from braindecode.preprocessing  import scale\n",
    "from EEGModels                  import EEGNet\n",
    "from matplotlib                 import pyplot as plt\n",
    "from mne                        import io\n",
    "from mne.datasets               import sample\n",
    "from pyriemann.estimation       import XdawnCovariances\n",
    "from pyriemann.tangentspace     import TangentSpace\n",
    "from pyriemann.utils.viz        import plot_confusion_matrix\n",
    "from sklearn.linear_model       import LogisticRegression\n",
    "from sklearn.model_selection    import train_test_split\n",
    "from sklearn.pipeline           import make_pipeline\n",
    "from sklearn.preprocessing      import normalize\n",
    "from tensorflow.keras           import backend as K\n",
    "from tensorflow.keras           import utils as np_utils\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils     import Sequence\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, x_set, y_set, batch_size):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        return batch_x, batch_y\n",
    "\n",
    "\n",
    "class BCI():\n",
    "    def __init__(self, subjects):\n",
    "        \"\"\"\n",
    "        Initializa loading dataset\n",
    "        \"\"\"\n",
    "        self.train_set,self.valid_set =  self.carrega_moaab(subjects)\n",
    "\n",
    "    def carrega_moaab(self,subjects,n_jobs=7,low_cut_hz=4.0,high_cut_hz=38.0,dataset_name=\"BNCI2014001\",offset = -0.5):\n",
    "        \"\"\"Load dataset\"\"\"\n",
    "        dataset = MOABBDataset(dataset_name=dataset_name, subject_ids=subjects)\n",
    "\n",
    "        #Preprocessamento transforma os dados\n",
    "        preprocessors = [\n",
    "            Preprocessor('pick_types', eeg=True, meg=False, stim=False),  # Defini os tipos de sensores que serão mantidos - somente eeg\n",
    "            Preprocessor(scale, factor=1e6, apply_on_array=True),         # Altera a escala dos dados\n",
    "            Preprocessor('filter', l_freq=low_cut_hz, h_freq=high_cut_hz),# Realiza o filtro de banda\n",
    "            #Preprocessor(exponential_moving_standardize,init_block_size=init_block_size)                 # Média movel distribuida ao longo dos dados\n",
    "        ]\n",
    "\n",
    "        # Transform the data\n",
    "        preprocess(concat_ds= dataset, preprocessors= preprocessors,save_dir=None,n_jobs=n_jobs)\n",
    "\n",
    "        # Obtem a frequencia padrão dos datasets e valida se todos são iguais\n",
    "        sfreq = dataset.datasets[0].raw.info['sfreq']\n",
    "        assert all([ds.raw.info['sfreq'] == sfreq for ds in dataset.datasets])\n",
    "\n",
    "        # Calcula quando começa de fato a execução do movimento.\n",
    "        trial_start_offset_samples = int(offset * sfreq)\n",
    "\n",
    "        windows_dataset = create_windows_from_events(\n",
    "            dataset,\n",
    "            trial_start_offset_samples=trial_start_offset_samples,\n",
    "            trial_stop_offset_samples=0,\n",
    "            preload=True,\n",
    "        )\n",
    "\n",
    "\n",
    "        #Divide os dados em treino e teste\n",
    "        splitted = windows_dataset.split('session')\n",
    "        train_set = splitted['session_T']\n",
    "        valid_set = splitted['session_E']\n",
    "\n",
    "        return (train_set,valid_set)\n",
    "\n",
    "    def cria_datasets(self,seg=4):\n",
    "        #Cria uma unica base de treino e uma unica base de teste\n",
    "        base_treino = []\n",
    "        for t_set in self.train_set.datasets:\n",
    "            base = t_set.windows.to_data_frame()\n",
    "            base['subject'] = t_set.description.subject\n",
    "            base['run'] = t_set.description.run\n",
    "            base_treino.append(base)\n",
    "            #break\n",
    "        t_df = pd.concat(base_treino).reset_index().drop('index',axis=1)\n",
    "\n",
    "        base_valid = []\n",
    "        for v_set in self.valid_set.datasets:\n",
    "            base = v_set.windows.to_data_frame()\n",
    "            base['subject'] = v_set.description.subject\n",
    "            base['run'] = v_set.description.run\n",
    "            base_valid.append(base)\n",
    "            #break\n",
    "        v_df = pd.concat(base_valid).reset_index().drop('index',axis=1)\n",
    "\n",
    "        campos = [\n",
    "                #'Cz','FCz','CPz'\n",
    "                #'Cz','C1','C2','FCz','FC1','FC2','CPz','CP1','CP2'\n",
    "                'Fz', 'FC3', 'FC1', 'FCz', 'FC2', 'FC4','C5', 'C3', 'C1', 'Cz', 'C2', 'C4', 'C6', 'CP3', 'CP1', 'CPz', 'CP2','CP4', 'P1', 'Pz', 'P2', 'POz'\n",
    "                ]\n",
    "        base_campos = campos.copy()\n",
    "        base_campos.extend(['time','condition','epoch','subject','run'])\n",
    "\n",
    "        #utilizando eletrodos mais proximos a região central do cérebro\n",
    "        df_train = t_df[base_campos].copy()\n",
    "        #df_train = df_train[(df_train['condition'] == 'left_hand') | (df_train['condition'] == 'tongue')].reset_index().drop('index',axis=1)\n",
    "\n",
    "        df_test = v_df[base_campos].copy()\n",
    "        #df_test = df_test[(df_test['condition'] == 'left_hand') | (df_test['condition'] == 'tongue')].reset_index().drop('index',axis=1)\n",
    "\n",
    "        tempo = (1125 * seg)\n",
    "\n",
    "        #cria agregação do tempo a cada 2 segundos\n",
    "        df_train['seg_agr'] = df_train['time'].apply(lambda x: (x+4)//tempo)\n",
    "        #cria agregação do tempo a cada 2 segundos\n",
    "\n",
    "        df_test['seg_agr'] = df_test['time'].apply(lambda x: (x+4)//tempo)\n",
    "\n",
    "        return (df_train, df_test)\n",
    "\n",
    "    def geraBase(self,df,campos,s_len,g_len):\n",
    "        dit = {'left_hand':0,'tongue':1,'right_hand':2,'feet':3}\n",
    "        dados = []\n",
    "        y = []\n",
    "        dit = {'left_hand':1,'tongue':2,'right_hand':3,'feet':4}\n",
    "        for gr, df_g in tqdm.tqdm(df.groupby(by=['subject','run','epoch','condition','seg_agr'])):\n",
    "            df_s = df_g.reset_index()\n",
    "            max = len(df_s)\n",
    "            if(max>=s_len):\n",
    "                for i in range(g_len):\n",
    "                    sp = df_s.sample(s_len).sort_values(by=['index'])\n",
    "                    dados.append(np.array(sp[campos].to_numpy().T))\n",
    "                    y.append(dit[gr[3]])\n",
    "        \n",
    "        y = np.array(y)\n",
    "        X = np.array(dados)\n",
    "        del(dados)\n",
    "        \n",
    "        return (X,y)\n",
    "\n",
    "    def train_val_test(self,df_train,df_test,campos,s_len , g_len):\n",
    "        X_train, Y_train  = self.geraBase(df_train,campos, s_len, g_len)\n",
    "        X_train,X_validate, Y_train,Y_validate  = train_test_split(X_train,Y_train, test_size=0.33,random_state=42,stratify=Y_train)\n",
    "\n",
    "        X_test,Y_test = self.geraBase(df_test,campos,s_len, g_len=1)\n",
    "\n",
    "        kernels, chans, samples = 1, X_train.shape[1], X_train.shape[2]\n",
    "\n",
    "        Y_train      = np_utils.to_categorical(Y_train-1)\n",
    "        Y_validate   = np_utils.to_categorical(Y_validate-1)\n",
    "        Y_test       = np_utils.to_categorical(Y_test-1)\n",
    "\n",
    "        X_train      = X_train.reshape(X_train.shape[0], chans, samples, kernels)\n",
    "        X_validate   = X_validate.reshape(X_validate.shape[0], chans, samples, kernels)\n",
    "        X_test       = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], kernels)\n",
    "\n",
    "        return (X_train,Y_train,X_validate,Y_validate,X_test,Y_test,chans, samples)\n",
    "\n",
    "    def train_NN(self,X_train,Y_train,X_validate,Y_validate,X_test,Y_test,chans, samples, kernLength = 64, F1 = 16, D = 2, F2 = 32,nb_classes = 4,batch_size = 64,epochs = 300,model_name='model.h5' ):\n",
    "        \n",
    "        train_gen = DataGenerator(X_train, Y_train, batch_size)\n",
    "        test_gen = DataGenerator(X_test, Y_test, batch_size)\n",
    "        val_gen = DataGenerator(X_validate, Y_validate, batch_size)\n",
    "\n",
    "        model = EEGNet(nb_classes = nb_classes, Chans = chans, Samples = samples, \n",
    "                    dropoutRate = 0.5, kernLength = kernLength, F1 = F1, D = D, F2 = F2, \n",
    "                    dropoutType = 'Dropout')\n",
    "\n",
    "        checkpointer = ModelCheckpoint(filepath=model_name, verbose=1,\n",
    "                                    save_best_only=True)\n",
    "\n",
    "        # compile the model and set the optimizers\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', \n",
    "                    metrics = ['accuracy'])\n",
    "\n",
    "        m_fit = model.fit(train_gen,\n",
    "                        batch_size = batch_size,\n",
    "                        epochs = epochs, \n",
    "                        validation_data=val_gen,\n",
    "                        callbacks=[checkpointer],\n",
    "                        )\n",
    "\n",
    "        # load optimal weights\n",
    "        model.load_weights(model_name)\n",
    "\n",
    "        probs       = model.predict(test_gen)\n",
    "        preds       = probs.argmax(axis = -1)  \n",
    "        acc         = np.mean(preds == Y_test.argmax(axis=-1))\n",
    "        return (model_name,acc,m_fit)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NC38kMQd9VaM"
   },
   "outputs": [],
   "source": [
    "bci = BCI([1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "k14uXSHE9aQl"
   },
   "outputs": [],
   "source": [
    "df_train, df_test = bci.cria_datasets(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0DAZLA219oWd",
    "outputId": "689b4f2d-4b0a-4c53-bbfb-2ec9c1ddbba5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1152/1152 [00:01<00:00, 730.59it/s]\n"
     ]
    }
   ],
   "source": [
    "X,y = bci.geraBase(df_train,['Cz','FCz'],1124,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cpd6IvZn-VoU"
   },
   "outputs": [],
   "source": [
    "#288 Capturas #Sensores(Cz) = 1 e #Tamanho da Amostra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DbWEa2uM-Ubu",
    "outputId": "9c48123f-e9bf-4069-daf5-bb59c75b1830"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(576, 2, 1124)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-AW6WN3z93Bk",
    "outputId": "65a7a848-4ba5-4255-9f62-1082262da17a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1124)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qimcq4gD76fx"
   },
   "outputs": [],
   "source": [
    "\n",
    "sujeitos = [[2]]#,[2],[3],[4],[5],[6],[7],[8],[9]]\n",
    "segundos = [4]#[4,2,1,0.5]\n",
    "batch_sizes = [512]#,128]\n",
    "modelo   = [(64,16,2,32)]\n",
    "params = {\n",
    "    #0.5: [(140,1),(140,10),(140,20),(140,30),(100,10),(100,20),(100,30),(100,40),(100,60)],\n",
    "    # 1: [(281,1),(281,10),(281,20),(281,30),(250,10),(250,20),(250,30),(250,40),(250,50),(250,60)],\n",
    "    # 2: [(562,1),(562,10),(562,20),(562,30),(500,10),(500,20),(500,30),(500,40),(500,50),(500,60)],\n",
    "    4: [(1000,50)]#,(1000,30),(1124,30)]#,(1124,30),(1000,30),(1124,40),(1000,40)]\n",
    "}\n",
    "path = ''#'trials\\\\exp3\\\\'\n",
    "import logging\n",
    "import time\n",
    "import json\n",
    "logging.basicConfig(filename=path+'models.log', encoding='utf-8', level=logging.DEBUG)\n",
    "campos = ['Fz', 'FC3', 'FC1', 'FCz', 'FC2', 'FC4','C5', 'C3', 'C1', 'Cz', 'C2', 'C4', 'C6', 'CP3', 'CP1', 'CPz', 'CP2','CP4', 'P1', 'Pz', 'P2', 'POz']\n",
    "\n",
    "\n",
    "\n",
    "for suj in sujeitos:\n",
    "    start_time_all = time.time()\n",
    "    bci = BCI(suj)\n",
    "    for seg in segundos:\n",
    "        df_train, df_test = bci.cria_datasets(seg)\n",
    "        for param in params[seg]:\n",
    "            for kernLength, F1, D , F2 in modelo:\n",
    "                for batch in batch_sizes:\n",
    "                    X_train,Y_train,X_validate,Y_validate,X_test,Y_test,chans,samples = bci.train_val_test(df_train,df_test,campos,s_len=param[0],g_len=param[1])\n",
    "                    model_name='V2model'+str(suj)+'_'+str(seg)+'_'+str(param[0])+'_'+str(param[1])+'_'+str(kernLength)+'_'+str(batch)+'_'+str(F1)+'_'+str(D)+'_'+str(F2)+'.h5' \n",
    "                    start_time = time.time()\n",
    "                    name,acc,model = bci.train_NN(X_train,Y_train,X_validate,Y_validate,X_test,Y_test,chans, samples,kernLength=kernLength, F1=F1, D=D , F2=F2,batch_size = batch,epochs = 300,model_name=path+model_name )\n",
    "                    end_time = time.time()\n",
    "                    time_elapsed = (end_time - start_time)\n",
    "                    time_elapsed_all = (end_time - start_time_all)\n",
    "                    \n",
    "                    #with open(path+'history\\\\'+model_name+\".json\", \"w\") as outfile:\n",
    "                    #    json.dump(model.history, outfile)\n",
    "\n",
    "                    logging.info(name+';'+str(acc)+';'+str(suj)+';'+str(seg)+';'+str(param[0])+';'+str(param[1])+';'+str(time_elapsed)+';'+str(time_elapsed_all)+';'+str(kernLength)+';'+str(batch)+';'+str(F1)+';'+str(D)+';'+str(F2))\n",
    "                    del(X_train)\n",
    "                    del(Y_train)\n",
    "                    del(X_validate)\n",
    "                    del(Y_validate)\n",
    "                    del(X_test)\n",
    "                    del(Y_test)\n",
    "                    del(chans)\n",
    "                    del(samples)\n",
    "        del(df_train)\n",
    "        del(df_test)\n",
    "    del(bci)\n",
    "\n",
    "logging.shutdown()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "BCI_DADOS.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
